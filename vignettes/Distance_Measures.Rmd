---
title: "Motivation"
author: "Dr. Simon MÃ¼ller"
date: "`r Sys.Date()`"
output:
  html_vignette:
    css: kable.css
    number_sections: yes
    toc: yes
vignette: >
  %\VignetteIndexEntry{Motivation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Objective

### Define a Distance Measure for Numerical and Categorical Features

Let $x_i = \left(x_i^1,\cdots,x_i^p\right)^T$ and $x_j = \left(x_j^1,\cdots,x_j^p\right)^T$
be the feature vectors of tow distinct patients $i$ and $j$. A first rough idea may be to calculate the $L_1-$norm of this two feature vectors:
$$
\|x_i - x_j\|_{L_1}=\frac{1}{p}\sum\limits_{k=1}^p|x_i^k - x_j^k|
$$

However, this naive approach arises from some problems:

1. This is well defined for numerical features, but what is about categorical features, e.g., gender (male/female). How shall we define the distance here?

2. $|x_i^k - x_j^k|$ is not scale-invariant, e.g., if one change from meters to centimeters, the contribution of this feature would explode by a factor of 100. Features with large values will dominate the distance. 

3. All features are treated the same way, e.g., lung cancer: smoker (no/yes) seems to be more important than the city the patient is living.

We propose a machine learning approach to overcoming these issues.

## Weighted Distance Measure

Two steps have to be done to overcome the above problems:

1. Generalize the L1-distance on the feature scale, such that it can deal with different variable types

2. Each feature needs to be weighted suitable 

  a. to rule out the dependency on the scale
  b. account for varying importance across features
  c. impact size of each feature

This leads to the following weighted distance measure:

$$
d(x_i, x_j) = \sum\limits_{k=1}^p |\alpha(x_i^k, x_j^k) d(x_i^k, x_j^k)|
$$

In the next section we will show how to get weights $\alpha(x_i^k, x_j^k)$ from our training data.

# Statistical Model

Let $\alpha(x_i^k, x_j^k)$ the weights and $d(x_i^k, x_j^k)$ the distance for feature $k$ and observation $i$ and $j$. We will define them as:

If feature $k$ is numerical, then

$$d(x_i^k, x_j^k) = x_i^k - x_j^k$$ and

$$\alpha(x_i^k, x_j^k) = \frac{\hat{\beta_k}}{\sum \hat{\beta_k}}$$

If feature $k$ is categorical, then

$$d(x_i^k, x_j^k) = 1 \text{ when } x_i^k = x_j^k \text{ else } 0$$ and
$$\alpha(x_i^k, x_j^k) = \frac{\hat{\beta_k}^i - \hat{\beta_k}^j}{\sum \hat{\beta_k}},$$
where $\hat{\beta_k}^k$ are coefficients of a regression model (linear, logistic, or CPH model).

# Random Forests

## Proximity Measure

Two observations $i$ and $j$ are more similar when the fraction of trees in which patient $i$ and $j$ share the same terminal node is close to one (Breiman, 2002).

$$
d(x_i, x_j)^2 = 1 - \frac{1}{M}\sum\limits_{t=1}^T 1_{[x_i \text{ and } x_j \text{ share the same terminal node in tree } t]},
$$
where $M$ is the number of trees that contain both observations, and $T$ is the number of trees. A drawback of this measure is that the decision is binary and possibly similar observations may be counted as not similar, e.g., assume a final cut off is always made around age 58, and observation 1 is 56 and observation is 60, then the distance of observation 1 to 2 is the same as from observation 1 to observation with age 80.

## Depth Measure: A modified proximity measure

The depth measure uses instead of the final node of each three the number of edges between two observations, which is finally averaged over all trees. More precisely, this distance measure is defined as:

$$
d(x_i, x_j) = \frac{1}{M}\sum\limits_{t=1}^T g_{ij},
$$
where $\omega \geq 0$ is a weighting factor and $g_{ij}$ is the number of edges between end nodes of observation $i$ and $j$ in tree $t$. More details can be found in the publication Englund and Verikas [A novel approach to estimate proximity in a random forest: An exploratory study](https://www.researchgate.net/publication/257404436_A_novel_approach_to_estimate_proximity_in_a_random_forest_An_exploratory_study).

